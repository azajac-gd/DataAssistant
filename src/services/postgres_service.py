import os
import psycopg2
import pandas as pd
import streamlit as st
from typing import List, Dict, Tuple, Set
from collections import defaultdict, deque
from dotenv import load_dotenv
import re
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError
import logging

load_dotenv()

def execute_ddl_and_save_data(ddl_text: str, data_tables: List[Dict]):
    try:
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST"),
            database=os.getenv("DATABASE"),
            user=os.getenv("USER"),
            password=os.getenv("PASSWORD")
        )
        cursor = conn.cursor()
        remove_existing_tables(cursor, conn)
        ddl_postgres = convert_mysql_to_postgres(ddl_text, use_pg_enums=True)
        ddl_cleaned = convert_with_cycle_support(ddl_postgres)
        cursor.execute(ddl_cleaned)

        for table in data_tables:
            table_name = table["table_name"]
            rows = table["rows"]
            if not rows:
                continue

            df = pd.DataFrame(rows)
            if df.empty:
                continue

            columns = list(df.columns)
            placeholders = ', '.join(['%s'] * len(columns))
            insert_query = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"

            for _, row in df.iterrows():
                values = tuple(row[col] for col in columns)
                cursor.execute(insert_query, values)

        conn.commit()
        cursor.close()
        conn.close()
        st.success("Tables created and data saved to PostgreSQL successfully!")
    except Exception as e:
        st.error(f"Error: {e}")

def remove_existing_tables(cursor, conn):
    try:
        cursor.execute("""
            DO $$
            DECLARE
                r RECORD;
            BEGIN
                FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
                    EXECUTE format('DROP TABLE IF EXISTS %I CASCADE', r.tablename);
                END LOOP;
                FOR r IN (SELECT typname FROM pg_type WHERE typnamespace = 'public'::regnamespace AND typtype = 'e') LOOP
                    EXECUTE format('DROP TYPE IF EXISTS %I CASCADE', r.typname);
                END LOOP;
            END $$;
        """)
        conn.commit()
    except Exception as e:
        st.error(f"Error removing existing tables: {e}")

def convert_enum_to_pgtype(enum_sql: str) -> Tuple[str, List[str]]:
    enums = []
    enum_types = []
    
    pattern = re.compile(r"(\w+)\s+ENUM\s*\(([^)]+)\)", re.IGNORECASE)
    def replace_enum(match):
        column_name = match.group(1)
        enum_values = match.group(2)
        type_name = f"{column_name.lower()}_enum"
        enum_type = f"CREATE TYPE {type_name} AS ENUM ({enum_values});"
        enums.append(enum_type)
        enum_types.append(type_name)
        return f"{column_name} {type_name}"
    
    converted = pattern.sub(replace_enum, enum_sql)
    return converted, enums

def convert_mysql_to_postgres(sql: str, use_pg_enums: bool = True) -> str:
    sql = re.sub(r"\bAUTO_INCREMENT\b", "GENERATED BY DEFAULT AS IDENTITY", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bDATETIME\b", "TIMESTAMP", sql, flags=re.IGNORECASE)
    sql = re.sub(r"ENGINE\s*=\s*\w+", "", sql, flags=re.IGNORECASE)
    sql = re.sub(r"DEFAULT\s+CHARSET\s*=\s*\w+", "", sql, flags=re.IGNORECASE)
    sql = re.sub(r"COLLATE\s*=\s*\w+", "", sql, flags=re.IGNORECASE)
    sql = sql.replace("`", '"')

    if use_pg_enums:
        sql, enum_defs = convert_enum_to_pgtype(sql)
        enum_sql = "\n".join(enum_defs) + "\n\n"
    else:
        sql = re.sub(r"\bENUM\s*\([^)]+\)", "VARCHAR", sql, flags=re.IGNORECASE)
        enum_sql = ""

    return enum_sql + sql


def extract_enum_types(sql: str) -> List[str]:
    pattern = re.compile(r'(CREATE TYPE \w+ AS ENUM\s*\(.*?\);)', re.DOTALL | re.IGNORECASE)
    return pattern.findall(sql)

def extract_table_definitions(sql: str) -> Dict[str, str]:
    table_defs = {}
    pattern = re.compile(r'CREATE TABLE (\w+)\s*\((.*?)\);', re.DOTALL | re.IGNORECASE)
    for match in pattern.finditer(sql):
        table_name = match.group(1)
        table_sql = f"CREATE TABLE {table_name} ({match.group(2).strip()});"
        table_defs[table_name] = table_sql
    return table_defs

def extract_foreign_keys(table_sql: str) -> List[Tuple[str, str]]:
    fks = []
    fk_pattern = re.compile(r'FOREIGN KEY\s*\(.*?\)\s*REFERENCES\s+(\w+)', re.IGNORECASE)
    for match in fk_pattern.finditer(table_sql):
        fks.append(match.group(1))
    return fks

def build_dependency_graph(table_defs: Dict[str, str]) -> Tuple[Dict[str, List[str]], List[Tuple[str, str, str]]]:
    graph = defaultdict(list)
    fk_constraints = []

    for table, sql in table_defs.items():
        for referenced in extract_foreign_keys(sql):
            graph[table].append(referenced)
            fk_constraints.append((table, referenced, extract_fk_line(sql, referenced)))

    return graph, fk_constraints

def extract_fk_line(sql: str, target_table: str) -> str:
    pattern = re.compile(r'(FOREIGN KEY\s*\(.*?\)\s*REFERENCES\s+' + target_table + r'\s*\(.*?\))', re.IGNORECASE)
    match = pattern.search(sql)
    return match.group(1) if match else ""

def detect_cycles(graph: Dict[str, List[str]]) -> Set[Tuple[str, str]]:
    visited = set()
    rec_stack = set()
    cyclic_edges = set()

    def visit(node):
        visited.add(node)
        rec_stack.add(node)
        for neighbor in graph.get(node, []):
            if neighbor not in visited:
                visit(neighbor)
            elif neighbor in rec_stack:
                cyclic_edges.add((node, neighbor))
        rec_stack.remove(node)

    for node in graph:
        if node not in visited:
            visit(node)

    return cyclic_edges

def topological_sort(tables: List[str], graph: Dict[str, List[str]], skip_edges: Set[Tuple[str, str]]) -> List[str]:
    indegree = defaultdict(int)
    for src in graph:
        for dst in graph[src]:
            if (src, dst) not in skip_edges:
                indegree[dst] += 1

    queue = deque([t for t in tables if indegree[t] == 0])
    result = []

    while queue:
        node = queue.popleft()
        result.append(node)
        for neighbor in graph.get(node, []):
            if (node, neighbor) in skip_edges:
                continue
            indegree[neighbor] -= 1
            if indegree[neighbor] == 0:
                queue.append(neighbor)

    if len(result) != len(tables):
        raise Exception("Unresolvable dependency detected (even after removing cycles).")

    return result

def convert_with_cycle_support(sql: str) -> str:
    enum_defs = extract_enum_types(sql)
    for enum_def in enum_defs:
        sql = sql.replace(enum_def, '')
    table_defs = extract_table_definitions(sql)
    graph, fk_constraints = build_dependency_graph(table_defs)
    cycles = detect_cycles(graph)

    altered_constraints = []
    for from_table, to_table, fk_sql in fk_constraints:
        if (from_table, to_table) in cycles:
            pattern = re.compile(r',?\s*' + re.escape(fk_sql), re.IGNORECASE)
            table_defs[from_table] = pattern.sub('', table_defs[from_table])
            altered_constraints.append((from_table, fk_sql))

    sorted_tables = topological_sort(list(table_defs.keys()), graph, cycles)[::-1]

    output_sql = []
    if enum_defs:
        output_sql.append("-- Enums")
        output_sql.extend(enum_defs)

    for table in sorted_tables:
        output_sql.append(table_defs[table])

    if altered_constraints:
        output_sql.append("\n-- Add deferred cyclic constraints --")
        for table, fk_sql in altered_constraints:
            alter_sql = f'ALTER TABLE {table} ADD {fk_sql} DEFERRABLE INITIALLY DEFERRED;'
            output_sql.append(alter_sql)
    return "\n\n".join(output_sql)


def get_engine():
    user = os.getenv("USER")
    password = os.getenv("PASSWORD")
    host = os.getenv("POSTGRES_HOST")
    database = os.getenv("DATABASE")

    return create_engine(f"postgresql+psycopg2://{user}:{password}@{host}/{database}")


def execute_sql(query: str) -> tuple[pd.DataFrame | None, str | None]:
    engine = get_engine()
    try:
        with engine.connect() as conn:
            df = pd.read_sql_query(query, conn)
            logging.info("Lack of errors in SQL execution")
            return df, None
    except SQLAlchemyError as e:
        logging.error(f"SQL execution error: {e}")
        return None, str(e)


